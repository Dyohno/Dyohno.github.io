[{"path":"/2024/11/02/Hadoop_weather统计/","content":"气象数据分组统计实验步骤: 在idea创建maven项目，并把三个weather项目的java文件写入, WeatherAnalysis.java, WeatherGroupingComparator.java, WeatherData.java. 将项目打包成jar,通过moberxterm把jar包和测试数据文件werther.txt传入到虚拟机 用hdfs命令把测试文件werther.txt 传到hdfs文件系统,用hadoop jar 命令执行jar包 在web上查看结果,发现内容为空,进行程序的检查.通过检查发现,map读取测试文件时,进行分割的语句:1String[] fields = value.toString().split(&quot;\\t&quot;); 是用的制表符\\t,而测试数据中数据的分割空格数量不一定是\\t,所以修改为\\s+表示一个或多个空格分隔. 再次进行2操作发现输出文件正确. 详细代码理解 首先三个程序之间的关系为: WeatherAnalysis.java 程序 WeatherAnalysis.java 程序为主程序,是进行mapreduce的核心程序. 其中 class WeatherMapper 继承mapper类实现对测试文件的读取与数据的存储与封装, 将测试数据城市,年份,月份,温度,存储在weatherData中作为键key,将温度存储在tempOut作为值value, 传入进行Shuffle 过程. 然后 WeatherReducer 继承 Reducer 类,负责接收到shuffle后的键值对, 并且对其进行求温度平均值和统计数量的操作,然后写入输出文件. WeatherGroupingComparator.java 程序 WeatherGroupingComparator.java 程序是一个自定义比较器文件, 属于shuffle的过程. 首先他接收到来自map后的键值对&lt;weatherData,tempOut&gt;后,将其强制转换为WeatherData的数据类型,在进行比较操作,比较的逻辑是先比城市,城市哈希值相同,则对年份进行比较,比较返回值会作为参数进行排序. WeatherData.java 程序 WeatherData.java 程序是对整个程序结构,用于存储和处理天气数据。 这个类用于 Hadoop MapReduce 程序中，作为键（key）或值（value）的数据类型。 程序运行结果截图: 程序代码:WeatherAnalysis.java:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package org.example;import org.example.WeatherData;import org.example.WeatherGroupingComparator;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class WeatherAnalysis &#123; public static class WeatherMapper extends Mapper&lt;Object, Text, org.example.WeatherData, IntWritable&gt; &#123; private org.example.WeatherData weatherData = new org.example.WeatherData(); private IntWritable tempOut = new IntWritable(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; // 输入格式：city year month temp String[] fields = value.toString().split(&quot;\\\\s+&quot;); if (fields.length == 4) &#123; weatherData.setCity(fields[0]); weatherData.setYear(Integer.parseInt(fields[1])); weatherData.setMonth(Integer.parseInt(fields[2])); int temp = Integer.parseInt(fields[3]); weatherData.setTemp(temp); tempOut.set(temp); context.write(weatherData, tempOut); &#125; &#125; &#125; public static class WeatherReducer extends Reducer&lt;org.example.WeatherData, IntWritable, Text, Text&gt; &#123; private Text outputKey = new Text(); private Text outputValue = new Text(); @Override protected void reduce(org.example.WeatherData key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; int count = 0; // 计算平均温度 for (IntWritable val : values) &#123; sum += val.get(); count++; &#125; double avg = sum / (double) count; outputKey.set(key.getCity() + &quot;-&quot; + key.getYear()); outputValue.set(String.format(&quot;平均温度: %.2f°C (共%d个月数据)&quot;, avg, count)); context.write(outputKey, outputValue); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, &quot;weather analysis&quot;); job.setJarByClass(org.example.WeatherAnalysis.class); job.setMapperClass(org.example.WeatherAnalysis.WeatherMapper.class); job.setReducerClass(org.example.WeatherAnalysis.WeatherReducer.class); // 设置GroupingComparator job.setGroupingComparatorClass(WeatherGroupingComparator.class); job.setMapOutputKeyClass(WeatherData.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; WeatherGroupingComparator.java:123456789101112131415161718192021222324package org.example;import org.example.WeatherData;import org.apache.hadoop.io.WritableComparable;import org.apache.hadoop.io.WritableComparator;public class WeatherGroupingComparator extends WritableComparator &#123; public WeatherGroupingComparator() &#123; super(org.example.WeatherData.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; org.example.WeatherData w1 = (org.example.WeatherData) a; org.example.WeatherData w2 = (org.example.WeatherData) b; // 只按城市和年份分组，忽略月份 int cityCompare = w1.getCity().compareTo(w2.getCity()); if (cityCompare != 0) &#123; return cityCompare; &#125; return Integer.compare(w1.getYear(), w2.getYear()); &#125;&#125; WeatherData.java:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package org.example;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;public class WeatherData implements WritableComparable&lt;org.example.WeatherData&gt; &#123; private String city; // 城市 private int year; // 年份 private int month; // 月份 private int temp; // 温度 public WeatherData() &#123;&#125; public WeatherData(String city, int year, int month, int temp) &#123; this.city = city; this.year = year; this.month = month; this.temp = temp; &#125; // Getter and Setter public String getCity() &#123; return city; &#125; public void setCity(String city) &#123; this.city = city; &#125; public int getYear() &#123; return year; &#125; public void setYear(int year) &#123; this.year = year; &#125; public int getMonth() &#123; return month; &#125; public void setMonth(int month) &#123; this.month = month; &#125; public int getTemp() &#123; return temp; &#125; public void setTemp(int temp) &#123; this.temp = temp; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(city); out.writeInt(year); out.writeInt(month); out.writeInt(temp); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.city = in.readUTF(); this.year = in.readInt(); this.month = in.readInt(); this.temp = in.readInt(); &#125; @Override public int compareTo(org.example.WeatherData other) &#123; // 按城市、年份、月份排序 int cityCompare = this.city.compareTo(other.city); if (cityCompare != 0) &#123; return cityCompare; &#125; int yearCompare = Integer.compare(this.year, other.year); if (yearCompare != 0) &#123; return yearCompare; &#125; return Integer.compare(this.month, other.month); &#125; @Override public String toString() &#123; return city + &quot;\\t&quot; + year + &quot;\\t&quot; + month + &quot;\\t&quot; + temp; &#125;&#125;"},{"title":"Hadoop项目:计算每组订单金额的最大值(包括新修改后的)","path":"/2024/10/30/Hadoop_ 计算每组订单金额最大值/","content":"1.原程序示例:&lt;1&gt;.GroupSort.java程序&lt;1.1&gt;具体代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package org.example;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.DoubleWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class GroupSort &#123;static class SortMapper extends Mapper&lt;LongWritable, Text,OrderBean, NullWritable&gt; &#123;OrderBean bean = new OrderBean();protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;String line = value.toString();String[] fields = line.split(&quot; &quot;);bean.set(new Text(fields[0]), new DoubleWritable(Double.parseDouble(fields[2])));context.write(bean, NullWritable.get());&#125;&#125;static class SortReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123;@Overrideprotected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; val, Context context)throws IOException, InterruptedException &#123;context.write(key, NullWritable.get());&#125;&#125;public static void main(String[] args) throws Exception &#123;Configuration conf = new Configuration();Job job = Job.getInstance(conf);job.setJarByClass(GroupSort.class); job.setOutputKeyClass(OrderBean.class); job.setOutputValueClass(NullWritable.class); job.setMapperClass(SortMapper.class); job.setReducerClass(SortReducer.class); job.setGroupingComparatorClass(MyGroupingComparator.class); job.setPartitionerClass(ItemIdPartitioner.class); job.setNumReduceTasks(2); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean res = job.waitForCompletion(true); System.exit(res ? 0 : 1); &#125;&#125; &lt;1.2&gt;GroupSort.java程序讲解:&lt;2&gt;.ItemIdPartitioner.java具体代码: 123456789101112package org.example;//自定义分区器import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.Partitioner;public class ItemIdPartitioner extends Partitioner&lt;OrderBean, NullWritable&gt;&#123;@Overridepublic int getPartition(OrderBean bean, NullWritable value, int numReduceTasks) &#123; return (bean.getItemid().hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125;"},{"title":"hexo个人博客搭建","path":"/2024/10/25/hexo搭建心得_Dyohno的第一篇博客/","content":"这是我的第一篇个人博客的文章,内容是关于我刚刚搭建的基于hexo框架以及部署在github服务器.纯属小白,如有错误或者建议还请指教. Node.js与git的安装与配置以及会遇到的问题Node.js的安装: 参考此链接: csdn2024最新版Node.js下载安装及环境配置教程 git的安装 git的安装非常简单,只需要无脑下一步 只需要注意加入桌面图标即可, 并且在属性_兼容性中修改为以管理员身份运行 hexo的安装 这里要参考的文章比较重要,注意更换国内镜像源 如果你仔细的安装Node.js安装教程进行, 一般在安装hexo时不会出现问题. 如果你出现了以下错误则说明Nodejs并没有配置好,请使用uninstall nodejs.exe卸载,并且删除c盘中关于nodejs的文件 1.npm既不是内部命令也不是外部程序 2.无法移除某某目录:一般是因为之前安装没有卸载干净再次进行了安装 … 个人在安装配置过程中遇到的问题: 在我安装好配置好之后,进行服务器部署时遇到的问题:如果你在进行ssh相关操作或者hexo d相关操作出现了错误,但是网页已经搭建完成, 可能的原因是因为开了科学上网访问GitHub,关闭即可."},{"title":"Hello World","path":"/2024/10/23/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]